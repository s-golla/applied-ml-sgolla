{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff3d59e",
   "metadata": {},
   "source": [
    "# P3: Building a Classifier - Titanic Survival Prediction\n",
    "\n",
    "**Author:** Saratchandra Golla  \n",
    "**Date:** 11/08/2025    \n",
    "**Dataset:** Titanic (from seaborn)     \n",
    "**Introduction:**\n",
    "This project aims to build and evaluate machine learning classification models to predict passenger survival on the Titanic, using the publicly available seaborn Titanic dataset. We will compare the performance of three different classifier types—Decision Tree (DT), Support Vector Machine (SVM), and a Neural Network (NN)—across three distinct feature sets to determine the most effective approach for this prediction task. The entire process will follow a structured methodology: data preparation, feature selection, model training, and performance evaluation using metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108d0c0",
   "metadata": {},
   "source": [
    "## Section 1: Import and Inspect the Data\n",
    "We begin by importing all necessary libraries in one place, as per standard practice, and then loading the titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2e8601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the Titanic dataset:\n",
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Display a few records to verify\n",
    "print(\"First 5 rows of the Titanic dataset:\")\n",
    "print(titanic.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2642c",
   "metadata": {},
   "source": [
    "## Section 2: Data Exploration and Preparation\n",
    "### 2.1 Handle Missing Values and Clean Data\n",
    "Missing values for continuous features like age will be imputed using the median, while missing values for categorical features like embark_town (or its coded counterpart embarked) will be filled using the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d905dab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after imputation:\n",
      "age            0\n",
      "embark_town    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values for 'age' using the median\n",
    "median_age = titanic['age'].median()\n",
    "titanic['age'] = titanic['age'].fillna(median_age)\n",
    "\n",
    "# Fill in missing values for 'embark_town' (and thus 'embarked') using the mode\n",
    "mode_embark = titanic['embark_town'].mode()[0]\n",
    "titanic['embark_town'] = titanic['embark_town'].fillna(mode_embark)\n",
    "\n",
    "# Check for remaining missing values in columns we plan to use\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(titanic[['age', 'embark_town']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d047b6",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering\n",
    "We create a new feature, family_size, and convert necessary categorical features (sex, embarked, alone) into numerical formats for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b40935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after feature engineering and mapping:\n",
      "   family_size  sex  embarked  alone\n",
      "0            2    0       2.0      0\n",
      "1            2    1       0.0      0\n",
      "2            1    1       2.0      1\n",
      "3            2    1       2.0      0\n",
      "4            1    0       2.0      1\n"
     ]
    }
   ],
   "source": [
    "# 1. Add family_size: sibsp (siblings/spouses) + parch (parents/children) + 1 (for the individual)\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# 2. Convert categorical 'sex' to numeric binary (male=0, female=1)\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# 3. Convert categorical 'embarked' to numeric (C=0, Q=1, S=2)\n",
    "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# 4. Binary feature - convert 'alone' to numeric binary (already True/False, convert to int 1/0)\n",
    "titanic['alone'] = titanic['alone'].astype(int)\n",
    "\n",
    "print(\"\\nData after feature engineering and mapping:\")\n",
    "print(titanic[['family_size', 'sex', 'embarked', 'alone']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a8d04",
   "metadata": {},
   "source": [
    "## Section 3: Feature Selection and Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fbbb8",
   "metadata": {},
   "source": [
    "### 3.1 Choose Features and Target\n",
    "We will evaluate model performance across three different input feature sets (X) with the target (y) being survived (categorical: 0 or 1).\n",
    "\n",
    "- **Case 1:** alone (A simple binary feature)\n",
    "- **Case 2:** age (A continuous, potentially predictive feature)\n",
    "- **Case 3:** age and family_size (A combination of two continuous/numerical features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c4e61",
   "metadata": {},
   "source": [
    "### 3.2 Define X (Features) and y (Target)\n",
    "We define the feature and target dataframes/series for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348428b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: Feature = alone\n",
    "# Select the feature 'alone' as input (double brackets for 2D DataFrame)\n",
    "X1 = titanic[['alone']]\n",
    "# Select 'survived' as the target (single brackets for 1D Series)\n",
    "y1 = titanic['survived']\n",
    "\n",
    "# Case 2: Feature = age\n",
    "# Select 'age' and drop rows where age is missing (dropna() is included for robustness, though age was imputed)\n",
    "X2 = titanic[['age']].dropna()\n",
    "# Select the matching 'survived' values using the same index\n",
    "y2 = titanic.loc[X2.index, 'survived']\n",
    "\n",
    "# Case 3: Features = age + family_size\n",
    "# Select both 'age' and 'family_size', and drop rows where missing (dropna() is included for robustness)\n",
    "X3 = titanic[['age', 'family_size']].dropna()\n",
    "# Select the corresponding 'survived' values for those rows\n",
    "y3 = titanic.loc[X3.index, 'survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b9848",
   "metadata": {},
   "source": [
    "### Reflection 3:\n",
    "\n",
    "- Why are these features selected? These features (alone, age, family_size) are selected because they are immediately available and intuitively relevant. For example, it is often hypothesized that women and children (related to age and potentially family_size) were prioritized for survival, while being alone might decrease one's chance of assistance.\n",
    "- Are there features that are likely to be highly predictive of survival? Features like sex and pclass (passenger class) are historically known to be highly predictive of survival based on the \"women and children first\" protocol and class-based rescue efforts. While we are exploring the required features, adding sex and pclass in a final \"best case\" would likely yield much higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
